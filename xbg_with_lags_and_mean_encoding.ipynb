{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from importlib import reload\n",
    "import date\n",
    "reload(date)\n",
    "from date import *\n",
    "import model\n",
    "reload(model)\n",
    "from model import *\n",
    "from metric import get_weights, NWRMSLE_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading batch from position 110000000, batch size 10000000...\n",
      "Filtering 10000000...\n",
      "Filtered 8659998, mapping...\n",
      "Mapped, reducing...\n",
      "Batch done.\n",
      "================================================================================\n",
      "Reading batch from position 120000000, batch size 10000000...\n",
      "Filtering 5497041...\n",
      "Filtered 5497041, mapping...\n",
      "Mapped, reducing...\n",
      "Batch done.\n",
      "End of dataset is found.\n"
     ]
    }
   ],
   "source": [
    "df = load_data_in_date_range('./data/train_processed.csv', '2017-04-04', '2017-08-15', 110000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reading additional datasets')\n",
    "items = pd.read_csv('./data/items_encoded.csv')\n",
    "stores = pd.read_csv('./data/stores_encoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "print('Converting data and joining additional data')\n",
    "df = convert_unit_sales(df)\n",
    "df = fill_empty_sales(df)\n",
    "df, cols_categories = extend_dataset(df, items, stores)\n",
    "gc.collect()\n",
    "!telegram-send \"Data is ready, starting lagged and mean extraction.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = items_cols + stores_cols + ['store_nbr','item_nbr','onpromotion']\n",
    "combinations = list(itertools.combinations(cat_features, 2)) + \n",
    "list(itertools.combinations(cat_features, 1)) + \n",
    "[['store_nbr','item_nbr','onpromotion'], ['store_nbr','item_nbr']]\n",
    "print('Adding mean target encoding, categories: {}'.format(combinations))\n",
    "df, cols_mean = add_mean_encoding(df, combinations)\n",
    "gc.collect()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Adding lagged features')\n",
    "df_prev = df[['item_nbr', 'store_nbr', 'date', 'unit_sales']]\n",
    "df, cols_lagged = fill_lagged(df, df_prev, 12, 18)\n",
    "del df_prev\n",
    "gc.collect()\n",
    "!telegram-send \"Lagged and mean extraction is ready, starting validation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = [('2017-05-01', '2017-06-15', '2017-06-16', '2017-06-30'),\n",
    "         ('2017-05-16', '2017-06-30', '2017-07-01', '2017-07-15'),\n",
    "         ('2017-06-01', '2017-07-15', '2017-07-16', '2017-07-31'),\n",
    "         ('2017-06-16', '2017-07-31', '2017-08-01', '2017-08-15')]\n",
    "\n",
    "param = {\n",
    "    'num_leaves':30,\n",
    "    'objective':'regression_l2',\n",
    "    'metric':'l2_root',\n",
    "    'num_threads':4\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "x_cols = cols_categories + ['onpromotion'] + cols_lagged + cols_mean\n",
    "print('X cols: {}'.format(x_cols))\n",
    "\n",
    "for train_start, train_end, test_start, test_end in folds:\n",
    "    print(\"Extracting fold...\")\n",
    "    train = extract_by_date(df, train_start, train_end)\n",
    "    test= extract_by_date(df, test_start, test_end)\n",
    "    \n",
    "    print(\"Preparing train...\")\n",
    "    train_X = train[x_cols]\n",
    "    train_y = train['unit_sales']\n",
    "    train_weights = get_weights(train['item_nbr'])\n",
    "    train_dataset = lgb.Dataset(train_X, label=train_y, weight=train_weights)\n",
    "    #del train\n",
    "    \n",
    "    print(\"Preparing test...\")\n",
    "    test_X = test[x_cols]\n",
    "    test_y = test['unit_sales']\n",
    "    test_weights = get_weights(test['item_nbr'])\n",
    "    test_dataset = lgb.Dataset(test_X, label=test_y, weight=test_weights, reference=train_dataset)\n",
    "    #del test\n",
    "    \n",
    "    print(\"Training!\")\n",
    "    bst = lgb.train(param, \n",
    "                    train_dataset, \n",
    "                    200,\n",
    "                    valid_sets=[test_dataset], \n",
    "                    early_stopping_rounds=10, \n",
    "                    verbose_eval=True, \n",
    "                    feature_name=x_cols, \n",
    "                    categorical_feature=ext_cols)\n",
    "    \n",
    "    test_y_pred = bst.predict(test_X)\n",
    "    error = NWRMSLE_log(test_y_pred, test_y, test_weights)\n",
    "    print('Validation error: {}'.format(error))\n",
    "    \n",
    "    results.append((bst.best_iteration, error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!telegram-send \"Mean lagged xgb validation finished. Results: $results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anatoliy\\Anaconda2\\envs\\py35\\lib\\site-packages\\lightgbm\\basic.py:1007: UserWarning: categorical_feature in Dataset is overrided. New categorical_feature is ['city', 'class', 'cluster', 'family', 'perishable', 'salary', 'state', 'type', 'weekday', 'weekend']\n",
      "  warnings.warn('categorical_feature in Dataset is overrided. New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    }
   ],
   "source": [
    "param['task'] = 'prediction'\n",
    "start = '2017-07-01'\n",
    "end = '2017-08-15'\n",
    "num_round = 100\n",
    "train = extract_by_date(df, start, end)\n",
    "train, ext_cols = extend_dataset(train, items, stores)\n",
    "x_cols = ext_cols + ['onpromotion'] + lagged_cols\n",
    "train_X = train[x_cols]\n",
    "train_y = train['unit_sales']\n",
    "train_weights = get_weights(train['item_nbr'])\n",
    "train_dataset = lgb.Dataset(train_X, label=train_y, weight=train_weights)\n",
    "bst = lgb.train(param, \n",
    "                train_dataset,\n",
    "                num_round,\n",
    "                feature_name=x_cols, \n",
    "                categorical_feature=ext_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('./data/test_processed.csv', dtype=types)\n",
    "\n",
    "df_prev = extract_by_date(df, '2017-07-25', '2017-08-15')\n",
    "test = fill_mean_encoding(test, df_prev)\n",
    "test = fill_lagged(test, df_prev, 12, 18)\n",
    "\n",
    "test, ext_cols = extend_dataset(test, items, stores)\n",
    "x_cols = ext_cols + ['onpromotion'] + lagged_cols\n",
    "test_X = test[x_cols]\n",
    "test['unit_sales'] = bst.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.sort_values(by='id', inplace=True)\n",
    "test.ix[test.unit_sales < 0, 'unit_sales'] = 0\n",
    "test[['id', 'unit_sales']].to_csv('./submissions/lgb_mean_encoded_lagged_0.54.csv.gz', float_format=\"%.4f\", index=False, compression='gzip')\n",
    "!telegram-send \"Submission done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
