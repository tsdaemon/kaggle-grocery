{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from date import *\n",
    "from importlib import reload\n",
    "import model\n",
    "reload(model)\n",
    "from model import *\n",
    "from metric import get_weights, NWRMSLE_log\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading batch from position 110000000, batch size 10000000...\n",
      "Filtering 10000000...\n",
      "Filtered 8659998, mapping...\n",
      "Mapped, reducing...\n",
      "Batch done.\n",
      "================================================================================\n",
      "Reading batch from position 120000000, batch size 10000000...\n",
      "Filtering 5497041...\n",
      "Filtered 5497041, mapping...\n",
      "Mapped, reducing...\n",
      "Batch done.\n",
      "End of dataset is found.\n"
     ]
    }
   ],
   "source": [
    "df = load_data_in_date_range('./data/train_processed.csv', '2017-04-04', '2017-08-15', 110000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>id</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>unit_sales_mean</th>\n",
       "      <th>unit_sales(t-12)</th>\n",
       "      <th>unit_sales(t-13)</th>\n",
       "      <th>unit_sales(t-14)</th>\n",
       "      <th>unit_sales(t-15)</th>\n",
       "      <th>unit_sales(t-16)</th>\n",
       "      <th>unit_sales(t-17)</th>\n",
       "      <th>unit_sales(t-18)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1576</td>\n",
       "      <td>46</td>\n",
       "      <td>1695836</td>\n",
       "      <td>113723318.0</td>\n",
       "      <td>3.983762</td>\n",
       "      <td>True</td>\n",
       "      <td>3.759474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.519459</td>\n",
       "      <td>3.736180</td>\n",
       "      <td>2.562543</td>\n",
       "      <td>2.337137</td>\n",
       "      <td>4.329911</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1576</td>\n",
       "      <td>46</td>\n",
       "      <td>1695837</td>\n",
       "      <td>113723319.0</td>\n",
       "      <td>2.596886</td>\n",
       "      <td>True</td>\n",
       "      <td>1.755228</td>\n",
       "      <td>1.568372</td>\n",
       "      <td>1.610999</td>\n",
       "      <td>1.583318</td>\n",
       "      <td>1.061135</td>\n",
       "      <td>2.051215</td>\n",
       "      <td>2.390052</td>\n",
       "      <td>2.177057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1576</td>\n",
       "      <td>46</td>\n",
       "      <td>1695840</td>\n",
       "      <td>113723320.0</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>True</td>\n",
       "      <td>1.730986</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.945910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1576</td>\n",
       "      <td>46</td>\n",
       "      <td>1695845</td>\n",
       "      <td>113723321.0</td>\n",
       "      <td>3.044523</td>\n",
       "      <td>True</td>\n",
       "      <td>3.595048</td>\n",
       "      <td>3.610918</td>\n",
       "      <td>3.784190</td>\n",
       "      <td>3.713572</td>\n",
       "      <td>3.135494</td>\n",
       "      <td>3.178054</td>\n",
       "      <td>3.970292</td>\n",
       "      <td>3.688879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1576</td>\n",
       "      <td>46</td>\n",
       "      <td>1695846</td>\n",
       "      <td>113723322.0</td>\n",
       "      <td>1.583318</td>\n",
       "      <td>True</td>\n",
       "      <td>1.075955</td>\n",
       "      <td>1.059444</td>\n",
       "      <td>1.029898</td>\n",
       "      <td>1.362078</td>\n",
       "      <td>1.232469</td>\n",
       "      <td>0.733813</td>\n",
       "      <td>1.667117</td>\n",
       "      <td>2.067159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date  store_nbr  item_nbr           id  unit_sales onpromotion  \\\n",
       "0  1576         46   1695836  113723318.0    3.983762        True   \n",
       "1  1576         46   1695837  113723319.0    2.596886        True   \n",
       "2  1576         46   1695840  113723320.0    1.098612        True   \n",
       "3  1576         46   1695845  113723321.0    3.044523        True   \n",
       "4  1576         46   1695846  113723322.0    1.583318        True   \n",
       "\n",
       "   unit_sales_mean  unit_sales(t-12)  unit_sales(t-13)  unit_sales(t-14)  \\\n",
       "0         3.759474          0.000000          2.519459          3.736180   \n",
       "1         1.755228          1.568372          1.610999          1.583318   \n",
       "2         1.730986          2.197225          2.708050          1.791759   \n",
       "3         3.595048          3.610918          3.784190          3.713572   \n",
       "4         1.075955          1.059444          1.029898          1.362078   \n",
       "\n",
       "   unit_sales(t-15)  unit_sales(t-16)  unit_sales(t-17)  unit_sales(t-18)  \n",
       "0          2.562543          2.337137          4.329911          0.000000  \n",
       "1          1.061135          2.051215          2.390052          2.177057  \n",
       "2          1.791759          1.945910          1.609438          1.945910  \n",
       "3          3.135494          3.178054          3.970292          3.688879  \n",
       "4          1.232469          0.733813          1.667117          2.067159  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = convert_unit_sales(df)\n",
    "df = fill_empty_sales(df)\n",
    "df, lagged_cols = add_lagged_and_mean_encoding(df, 12, 18)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "items = pd.read_csv('./data/items_encoded.csv')\n",
    "stores = pd.read_csv('./data/stores_encoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!telegram-send \"Data is ready\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folds = [('2017-05-01', '2017-06-15', '2017-06-16', '2017-06-30'),\n",
    "         ('2017-05-16', '2017-06-30', '2017-07-01', '2017-07-15'),\n",
    "         ('2017-06-01', '2017-07-15', '2017-07-16', '2017-07-31'),\n",
    "         ('2017-06-16', '2017-07-31', '2017-08-01', '2017-08-15')]\n",
    "\n",
    "param = {\n",
    "    'num_leaves':30,\n",
    "    'objective':'regression_l2',\n",
    "    'metric':'l2_root',\n",
    "    'num_threads':4\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting fold...\n",
      "Preparing train...\n",
      "Preparing test...\n",
      "Training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anatoliy\\Anaconda2\\envs\\py35\\lib\\site-packages\\lightgbm\\basic.py:1007: UserWarning: categorical_feature in Dataset is overrided. New categorical_feature is ['city', 'class', 'cluster', 'family', 'perishable', 'salary', 'state', 'type', 'weekday', 'weekend']\n",
      "  warnings.warn('categorical_feature in Dataset is overrided. New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's rmse: 0.95709\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tvalid_0's rmse: 0.892796\n",
      "[3]\tvalid_0's rmse: 0.837109\n",
      "[4]\tvalid_0's rmse: 0.789127\n",
      "[5]\tvalid_0's rmse: 0.748215\n",
      "[6]\tvalid_0's rmse: 0.713331\n",
      "[7]\tvalid_0's rmse: 0.683847\n",
      "[8]\tvalid_0's rmse: 0.659181\n",
      "[9]\tvalid_0's rmse: 0.638232\n",
      "[10]\tvalid_0's rmse: 0.620906\n",
      "[11]\tvalid_0's rmse: 0.606442\n",
      "[12]\tvalid_0's rmse: 0.594603\n",
      "[13]\tvalid_0's rmse: 0.584795\n",
      "[14]\tvalid_0's rmse: 0.576736\n",
      "[15]\tvalid_0's rmse: 0.570162\n",
      "[16]\tvalid_0's rmse: 0.564794\n",
      "[17]\tvalid_0's rmse: 0.560443\n",
      "[18]\tvalid_0's rmse: 0.556923\n",
      "[19]\tvalid_0's rmse: 0.554025\n",
      "[20]\tvalid_0's rmse: 0.55167\n",
      "[21]\tvalid_0's rmse: 0.549768\n",
      "[22]\tvalid_0's rmse: 0.548235\n",
      "[23]\tvalid_0's rmse: 0.547009\n",
      "[24]\tvalid_0's rmse: 0.545991\n",
      "[25]\tvalid_0's rmse: 0.545184\n",
      "[26]\tvalid_0's rmse: 0.544562\n",
      "[27]\tvalid_0's rmse: 0.543974\n",
      "[28]\tvalid_0's rmse: 0.543547\n",
      "[29]\tvalid_0's rmse: 0.54322\n",
      "[30]\tvalid_0's rmse: 0.542888\n",
      "[31]\tvalid_0's rmse: 0.542634\n",
      "[32]\tvalid_0's rmse: 0.542418\n",
      "[33]\tvalid_0's rmse: 0.542232\n",
      "[34]\tvalid_0's rmse: 0.542105\n",
      "[35]\tvalid_0's rmse: 0.541967\n",
      "[36]\tvalid_0's rmse: 0.541862\n",
      "[37]\tvalid_0's rmse: 0.54181\n",
      "[38]\tvalid_0's rmse: 0.541704\n",
      "[39]\tvalid_0's rmse: 0.541624\n",
      "[40]\tvalid_0's rmse: 0.541584\n",
      "[41]\tvalid_0's rmse: 0.541542\n",
      "[42]\tvalid_0's rmse: 0.54149\n",
      "[43]\tvalid_0's rmse: 0.541451\n",
      "[44]\tvalid_0's rmse: 0.541427\n",
      "[45]\tvalid_0's rmse: 0.541387\n",
      "[46]\tvalid_0's rmse: 0.541361\n",
      "[47]\tvalid_0's rmse: 0.541312\n",
      "[48]\tvalid_0's rmse: 0.541277\n",
      "[49]\tvalid_0's rmse: 0.541248\n",
      "[50]\tvalid_0's rmse: 0.541219\n",
      "[51]\tvalid_0's rmse: 0.541212\n",
      "[52]\tvalid_0's rmse: 0.541212\n",
      "[53]\tvalid_0's rmse: 0.541215\n",
      "[54]\tvalid_0's rmse: 0.541166\n",
      "[55]\tvalid_0's rmse: 0.541111\n",
      "[56]\tvalid_0's rmse: 0.541073\n",
      "[57]\tvalid_0's rmse: 0.540992\n",
      "[58]\tvalid_0's rmse: 0.540952\n",
      "[59]\tvalid_0's rmse: 0.540948\n",
      "[60]\tvalid_0's rmse: 0.540942\n",
      "[61]\tvalid_0's rmse: 0.540925\n",
      "[62]\tvalid_0's rmse: 0.540906\n",
      "[63]\tvalid_0's rmse: 0.540856\n",
      "[64]\tvalid_0's rmse: 0.540813\n",
      "[65]\tvalid_0's rmse: 0.540782\n",
      "[66]\tvalid_0's rmse: 0.540751\n",
      "[67]\tvalid_0's rmse: 0.54072\n",
      "[68]\tvalid_0's rmse: 0.540699\n",
      "[69]\tvalid_0's rmse: 0.540691\n",
      "[70]\tvalid_0's rmse: 0.540659\n",
      "[71]\tvalid_0's rmse: 0.540649\n",
      "[72]\tvalid_0's rmse: 0.540622\n",
      "[73]\tvalid_0's rmse: 0.540569\n",
      "[74]\tvalid_0's rmse: 0.540538\n",
      "[75]\tvalid_0's rmse: 0.540551\n",
      "[76]\tvalid_0's rmse: 0.540564\n",
      "[77]\tvalid_0's rmse: 0.540565\n",
      "[78]\tvalid_0's rmse: 0.540544\n",
      "[79]\tvalid_0's rmse: 0.540524\n",
      "[80]\tvalid_0's rmse: 0.54051\n",
      "[81]\tvalid_0's rmse: 0.540442\n",
      "[82]\tvalid_0's rmse: 0.540413\n",
      "[83]\tvalid_0's rmse: 0.540396\n",
      "[84]\tvalid_0's rmse: 0.540403\n",
      "[85]\tvalid_0's rmse: 0.540396\n",
      "[86]\tvalid_0's rmse: 0.54037\n",
      "[87]\tvalid_0's rmse: 0.540367\n",
      "[88]\tvalid_0's rmse: 0.540367\n",
      "[89]\tvalid_0's rmse: 0.54033\n",
      "[90]\tvalid_0's rmse: 0.540305\n",
      "[91]\tvalid_0's rmse: 0.54025\n",
      "[92]\tvalid_0's rmse: 0.540242\n",
      "[93]\tvalid_0's rmse: 0.540228\n",
      "[94]\tvalid_0's rmse: 0.540222\n",
      "[95]\tvalid_0's rmse: 0.540203\n",
      "[96]\tvalid_0's rmse: 0.540193\n",
      "[97]\tvalid_0's rmse: 0.540183\n",
      "[98]\tvalid_0's rmse: 0.540174\n",
      "[99]\tvalid_0's rmse: 0.540182\n",
      "[100]\tvalid_0's rmse: 0.540167\n",
      "[101]\tvalid_0's rmse: 0.540172\n",
      "[102]\tvalid_0's rmse: 0.540185\n",
      "[103]\tvalid_0's rmse: 0.540181\n",
      "[104]\tvalid_0's rmse: 0.540173\n",
      "[105]\tvalid_0's rmse: 0.540156\n",
      "[106]\tvalid_0's rmse: 0.540129\n",
      "[107]\tvalid_0's rmse: 0.54012\n",
      "[108]\tvalid_0's rmse: 0.540112\n",
      "[109]\tvalid_0's rmse: 0.5401\n",
      "[110]\tvalid_0's rmse: 0.540112\n",
      "[111]\tvalid_0's rmse: 0.540111\n",
      "[112]\tvalid_0's rmse: 0.5401\n",
      "[113]\tvalid_0's rmse: 0.540103\n",
      "[114]\tvalid_0's rmse: 0.540112\n",
      "[115]\tvalid_0's rmse: 0.540092\n",
      "[116]\tvalid_0's rmse: 0.54009\n",
      "[117]\tvalid_0's rmse: 0.540069\n",
      "[118]\tvalid_0's rmse: 0.54006\n",
      "[119]\tvalid_0's rmse: 0.540056\n",
      "[120]\tvalid_0's rmse: 0.540055\n",
      "[121]\tvalid_0's rmse: 0.540021\n",
      "[122]\tvalid_0's rmse: 0.54003\n",
      "[123]\tvalid_0's rmse: 0.540007\n",
      "[124]\tvalid_0's rmse: 0.540012\n",
      "[125]\tvalid_0's rmse: 0.539973\n",
      "[126]\tvalid_0's rmse: 0.539974\n",
      "[127]\tvalid_0's rmse: 0.539965\n",
      "[128]\tvalid_0's rmse: 0.539956\n",
      "[129]\tvalid_0's rmse: 0.539956\n",
      "[130]\tvalid_0's rmse: 0.539953\n",
      "[131]\tvalid_0's rmse: 0.539953\n",
      "[132]\tvalid_0's rmse: 0.539933\n",
      "[133]\tvalid_0's rmse: 0.539931\n",
      "[134]\tvalid_0's rmse: 0.539934\n",
      "[135]\tvalid_0's rmse: 0.539936\n",
      "[136]\tvalid_0's rmse: 0.539938\n",
      "[137]\tvalid_0's rmse: 0.539904\n",
      "[138]\tvalid_0's rmse: 0.539896\n",
      "[139]\tvalid_0's rmse: 0.539889\n",
      "[140]\tvalid_0's rmse: 0.539879\n",
      "[141]\tvalid_0's rmse: 0.53987\n",
      "[142]\tvalid_0's rmse: 0.539873\n",
      "[143]\tvalid_0's rmse: 0.539859\n",
      "[144]\tvalid_0's rmse: 0.53985\n",
      "[145]\tvalid_0's rmse: 0.539846\n",
      "[146]\tvalid_0's rmse: 0.539834\n",
      "[147]\tvalid_0's rmse: 0.539828\n",
      "[148]\tvalid_0's rmse: 0.539822\n",
      "[149]\tvalid_0's rmse: 0.539818\n",
      "[150]\tvalid_0's rmse: 0.539804\n",
      "[151]\tvalid_0's rmse: 0.539801\n",
      "[152]\tvalid_0's rmse: 0.5398\n",
      "[153]\tvalid_0's rmse: 0.539812\n",
      "[154]\tvalid_0's rmse: 0.53981\n",
      "[155]\tvalid_0's rmse: 0.539799\n",
      "[156]\tvalid_0's rmse: 0.539788\n",
      "[157]\tvalid_0's rmse: 0.539793\n",
      "[158]\tvalid_0's rmse: 0.539787\n",
      "[159]\tvalid_0's rmse: 0.539785\n",
      "[160]\tvalid_0's rmse: 0.539779\n",
      "[161]\tvalid_0's rmse: 0.539773\n",
      "[162]\tvalid_0's rmse: 0.539766\n",
      "[163]\tvalid_0's rmse: 0.539765\n",
      "[164]\tvalid_0's rmse: 0.539757\n",
      "[165]\tvalid_0's rmse: 0.539752\n",
      "[166]\tvalid_0's rmse: 0.539746\n",
      "[167]\tvalid_0's rmse: 0.539736\n",
      "[168]\tvalid_0's rmse: 0.539724\n",
      "[169]\tvalid_0's rmse: 0.539721\n",
      "[170]\tvalid_0's rmse: 0.539724\n",
      "[171]\tvalid_0's rmse: 0.539712\n",
      "[172]\tvalid_0's rmse: 0.539707\n",
      "[173]\tvalid_0's rmse: 0.539652\n",
      "[174]\tvalid_0's rmse: 0.53965\n",
      "[175]\tvalid_0's rmse: 0.53965\n",
      "[176]\tvalid_0's rmse: 0.539647\n",
      "[177]\tvalid_0's rmse: 0.539651\n",
      "[178]\tvalid_0's rmse: 0.539624\n",
      "[179]\tvalid_0's rmse: 0.539622\n",
      "[180]\tvalid_0's rmse: 0.539625\n",
      "[181]\tvalid_0's rmse: 0.539632\n",
      "[182]\tvalid_0's rmse: 0.539627\n",
      "[183]\tvalid_0's rmse: 0.539615\n",
      "[184]\tvalid_0's rmse: 0.53961\n",
      "[185]\tvalid_0's rmse: 0.539612\n",
      "[186]\tvalid_0's rmse: 0.539611\n",
      "[187]\tvalid_0's rmse: 0.539613\n",
      "[188]\tvalid_0's rmse: 0.539603\n",
      "[189]\tvalid_0's rmse: 0.539605\n",
      "[190]\tvalid_0's rmse: 0.539607\n",
      "[191]\tvalid_0's rmse: 0.539607\n",
      "[192]\tvalid_0's rmse: 0.539604\n",
      "[193]\tvalid_0's rmse: 0.539601\n",
      "[194]\tvalid_0's rmse: 0.539546\n",
      "[195]\tvalid_0's rmse: 0.539534\n",
      "[196]\tvalid_0's rmse: 0.53952\n",
      "[197]\tvalid_0's rmse: 0.539518\n",
      "[198]\tvalid_0's rmse: 0.539516\n",
      "[199]\tvalid_0's rmse: 0.539515\n",
      "[200]\tvalid_0's rmse: 0.539522\n",
      "Validation error: 0.539522393256322\n",
      "Extracting fold...\n",
      "Preparing train...\n",
      "Preparing test...\n",
      "Training!\n",
      "[1]\tvalid_0's rmse: 0.984626\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tvalid_0's rmse: 0.920379\n",
      "[3]\tvalid_0's rmse: 0.864648\n",
      "[4]\tvalid_0's rmse: 0.816595\n",
      "[5]\tvalid_0's rmse: 0.775247\n",
      "[6]\tvalid_0's rmse: 0.74004\n",
      "[7]\tvalid_0's rmse: 0.710243\n",
      "[8]\tvalid_0's rmse: 0.684832\n",
      "[9]\tvalid_0's rmse: 0.663465\n",
      "[10]\tvalid_0's rmse: 0.645412\n",
      "[11]\tvalid_0's rmse: 0.630192\n",
      "[12]\tvalid_0's rmse: 0.617513\n",
      "[13]\tvalid_0's rmse: 0.606867\n",
      "[14]\tvalid_0's rmse: 0.597975\n",
      "[15]\tvalid_0's rmse: 0.590533\n",
      "[16]\tvalid_0's rmse: 0.584387\n",
      "[17]\tvalid_0's rmse: 0.579294\n",
      "[18]\tvalid_0's rmse: 0.575028\n",
      "[19]\tvalid_0's rmse: 0.571455\n",
      "[20]\tvalid_0's rmse: 0.568423\n",
      "[21]\tvalid_0's rmse: 0.565847\n",
      "[22]\tvalid_0's rmse: 0.563672\n",
      "[23]\tvalid_0's rmse: 0.561888\n",
      "[24]\tvalid_0's rmse: 0.560384\n",
      "[25]\tvalid_0's rmse: 0.559113\n",
      "[26]\tvalid_0's rmse: 0.55805\n",
      "[27]\tvalid_0's rmse: 0.55713\n",
      "[28]\tvalid_0's rmse: 0.556318\n",
      "[29]\tvalid_0's rmse: 0.555584\n",
      "[30]\tvalid_0's rmse: 0.554967\n",
      "[31]\tvalid_0's rmse: 0.554395\n",
      "[32]\tvalid_0's rmse: 0.553908\n",
      "[33]\tvalid_0's rmse: 0.553482\n",
      "[34]\tvalid_0's rmse: 0.553109\n",
      "[35]\tvalid_0's rmse: 0.552742\n",
      "[36]\tvalid_0's rmse: 0.552445\n",
      "[37]\tvalid_0's rmse: 0.552148\n",
      "[38]\tvalid_0's rmse: 0.551881\n",
      "[39]\tvalid_0's rmse: 0.551694\n",
      "[40]\tvalid_0's rmse: 0.551501\n",
      "[41]\tvalid_0's rmse: 0.551326\n",
      "[42]\tvalid_0's rmse: 0.551165\n",
      "[43]\tvalid_0's rmse: 0.551028\n",
      "[44]\tvalid_0's rmse: 0.550895\n",
      "[45]\tvalid_0's rmse: 0.550787\n",
      "[46]\tvalid_0's rmse: 0.55067\n",
      "[47]\tvalid_0's rmse: 0.550553\n",
      "[48]\tvalid_0's rmse: 0.550419\n",
      "[49]\tvalid_0's rmse: 0.550263\n",
      "[50]\tvalid_0's rmse: 0.550206\n",
      "[51]\tvalid_0's rmse: 0.550113\n",
      "[52]\tvalid_0's rmse: 0.550038\n",
      "[53]\tvalid_0's rmse: 0.549988\n",
      "[54]\tvalid_0's rmse: 0.549917\n",
      "[55]\tvalid_0's rmse: 0.549832\n",
      "[56]\tvalid_0's rmse: 0.549747\n",
      "[57]\tvalid_0's rmse: 0.549699\n",
      "[58]\tvalid_0's rmse: 0.549619\n",
      "[59]\tvalid_0's rmse: 0.549591\n",
      "[60]\tvalid_0's rmse: 0.549521\n",
      "[61]\tvalid_0's rmse: 0.549482\n",
      "[62]\tvalid_0's rmse: 0.549445\n",
      "[63]\tvalid_0's rmse: 0.549366\n",
      "[64]\tvalid_0's rmse: 0.549346\n",
      "[65]\tvalid_0's rmse: 0.549287\n",
      "[66]\tvalid_0's rmse: 0.549267\n",
      "[67]\tvalid_0's rmse: 0.54921\n",
      "[68]\tvalid_0's rmse: 0.549188\n",
      "[69]\tvalid_0's rmse: 0.549166\n",
      "[70]\tvalid_0's rmse: 0.54913\n",
      "[71]\tvalid_0's rmse: 0.549103\n",
      "[72]\tvalid_0's rmse: 0.549065\n",
      "[73]\tvalid_0's rmse: 0.548988\n",
      "[74]\tvalid_0's rmse: 0.548982\n",
      "[75]\tvalid_0's rmse: 0.548974\n",
      "[76]\tvalid_0's rmse: 0.548971\n",
      "[77]\tvalid_0's rmse: 0.548944\n",
      "[78]\tvalid_0's rmse: 0.548912\n",
      "[79]\tvalid_0's rmse: 0.548899\n",
      "[80]\tvalid_0's rmse: 0.54889\n",
      "[81]\tvalid_0's rmse: 0.54888\n",
      "[82]\tvalid_0's rmse: 0.54884\n",
      "[83]\tvalid_0's rmse: 0.548803\n",
      "[84]\tvalid_0's rmse: 0.548743\n",
      "[85]\tvalid_0's rmse: 0.548739\n",
      "[86]\tvalid_0's rmse: 0.548731\n",
      "[87]\tvalid_0's rmse: 0.548708\n",
      "[88]\tvalid_0's rmse: 0.548694\n",
      "[89]\tvalid_0's rmse: 0.54862\n",
      "[90]\tvalid_0's rmse: 0.548603\n",
      "[91]\tvalid_0's rmse: 0.548589\n",
      "[92]\tvalid_0's rmse: 0.548565\n",
      "[93]\tvalid_0's rmse: 0.548556\n",
      "[94]\tvalid_0's rmse: 0.548548\n",
      "[95]\tvalid_0's rmse: 0.548518\n",
      "[96]\tvalid_0's rmse: 0.548505\n",
      "[97]\tvalid_0's rmse: 0.548487\n",
      "[98]\tvalid_0's rmse: 0.548465\n",
      "[99]\tvalid_0's rmse: 0.548433\n",
      "[100]\tvalid_0's rmse: 0.548433\n",
      "[101]\tvalid_0's rmse: 0.548409\n",
      "[102]\tvalid_0's rmse: 0.5484\n",
      "[103]\tvalid_0's rmse: 0.548391\n",
      "[104]\tvalid_0's rmse: 0.54838\n",
      "[105]\tvalid_0's rmse: 0.548333\n",
      "[106]\tvalid_0's rmse: 0.548332\n",
      "[107]\tvalid_0's rmse: 0.548315\n",
      "[108]\tvalid_0's rmse: 0.5483\n",
      "[109]\tvalid_0's rmse: 0.548291\n",
      "[110]\tvalid_0's rmse: 0.548275\n",
      "[111]\tvalid_0's rmse: 0.548245\n",
      "[112]\tvalid_0's rmse: 0.548181\n",
      "[113]\tvalid_0's rmse: 0.548166\n",
      "[114]\tvalid_0's rmse: 0.548162\n",
      "[115]\tvalid_0's rmse: 0.548159\n",
      "[116]\tvalid_0's rmse: 0.548138\n",
      "[117]\tvalid_0's rmse: 0.548134\n",
      "[118]\tvalid_0's rmse: 0.548124\n",
      "[119]\tvalid_0's rmse: 0.548123\n",
      "[120]\tvalid_0's rmse: 0.548113\n",
      "[121]\tvalid_0's rmse: 0.548108\n",
      "[122]\tvalid_0's rmse: 0.548099\n",
      "[123]\tvalid_0's rmse: 0.548079\n",
      "[124]\tvalid_0's rmse: 0.548076\n",
      "[125]\tvalid_0's rmse: 0.548071\n",
      "[126]\tvalid_0's rmse: 0.548069\n",
      "[127]\tvalid_0's rmse: 0.548052\n",
      "[128]\tvalid_0's rmse: 0.548039\n",
      "[129]\tvalid_0's rmse: 0.548038\n",
      "[130]\tvalid_0's rmse: 0.548043\n",
      "[131]\tvalid_0's rmse: 0.548026\n",
      "[132]\tvalid_0's rmse: 0.548015\n",
      "[133]\tvalid_0's rmse: 0.548008\n",
      "[134]\tvalid_0's rmse: 0.547969\n",
      "[135]\tvalid_0's rmse: 0.547957\n",
      "[136]\tvalid_0's rmse: 0.547948\n",
      "[137]\tvalid_0's rmse: 0.547941\n",
      "[138]\tvalid_0's rmse: 0.547934\n",
      "[139]\tvalid_0's rmse: 0.547932\n",
      "[140]\tvalid_0's rmse: 0.547927\n",
      "[141]\tvalid_0's rmse: 0.547927\n",
      "[142]\tvalid_0's rmse: 0.547921\n",
      "[143]\tvalid_0's rmse: 0.54792\n",
      "[144]\tvalid_0's rmse: 0.547901\n",
      "[145]\tvalid_0's rmse: 0.547901\n",
      "[146]\tvalid_0's rmse: 0.547899\n",
      "[147]\tvalid_0's rmse: 0.547897\n",
      "[148]\tvalid_0's rmse: 0.547904\n",
      "[149]\tvalid_0's rmse: 0.5479\n",
      "[150]\tvalid_0's rmse: 0.547898\n",
      "[151]\tvalid_0's rmse: 0.547867\n",
      "[152]\tvalid_0's rmse: 0.547851\n",
      "[153]\tvalid_0's rmse: 0.547847\n",
      "[154]\tvalid_0's rmse: 0.547847\n",
      "[155]\tvalid_0's rmse: 0.547843\n",
      "[156]\tvalid_0's rmse: 0.547839\n",
      "[157]\tvalid_0's rmse: 0.547821\n",
      "[158]\tvalid_0's rmse: 0.547811\n",
      "[159]\tvalid_0's rmse: 0.54779\n",
      "[160]\tvalid_0's rmse: 0.547783\n",
      "[161]\tvalid_0's rmse: 0.54778\n",
      "[162]\tvalid_0's rmse: 0.54778\n",
      "[163]\tvalid_0's rmse: 0.547763\n",
      "[164]\tvalid_0's rmse: 0.54775\n",
      "[165]\tvalid_0's rmse: 0.547746\n",
      "[166]\tvalid_0's rmse: 0.547744\n",
      "[167]\tvalid_0's rmse: 0.547716\n",
      "[168]\tvalid_0's rmse: 0.547714\n",
      "[169]\tvalid_0's rmse: 0.547703\n",
      "[170]\tvalid_0's rmse: 0.547689\n",
      "[171]\tvalid_0's rmse: 0.54769\n",
      "[172]\tvalid_0's rmse: 0.547679\n",
      "[173]\tvalid_0's rmse: 0.547673\n",
      "[174]\tvalid_0's rmse: 0.547676\n",
      "[175]\tvalid_0's rmse: 0.547636\n",
      "[176]\tvalid_0's rmse: 0.547634\n",
      "[177]\tvalid_0's rmse: 0.547623\n",
      "[178]\tvalid_0's rmse: 0.547617\n",
      "[179]\tvalid_0's rmse: 0.547614\n",
      "[180]\tvalid_0's rmse: 0.547609\n",
      "[181]\tvalid_0's rmse: 0.547604\n",
      "[182]\tvalid_0's rmse: 0.547608\n",
      "[183]\tvalid_0's rmse: 0.547597\n",
      "[184]\tvalid_0's rmse: 0.547583\n",
      "[185]\tvalid_0's rmse: 0.54758\n",
      "[186]\tvalid_0's rmse: 0.547578\n",
      "[187]\tvalid_0's rmse: 0.547552\n",
      "[188]\tvalid_0's rmse: 0.547555\n",
      "[189]\tvalid_0's rmse: 0.547555\n",
      "[190]\tvalid_0's rmse: 0.547533\n",
      "[191]\tvalid_0's rmse: 0.547534\n",
      "[192]\tvalid_0's rmse: 0.547528\n",
      "[193]\tvalid_0's rmse: 0.547527\n",
      "[194]\tvalid_0's rmse: 0.547527\n",
      "[195]\tvalid_0's rmse: 0.547523\n",
      "[196]\tvalid_0's rmse: 0.547526\n",
      "[197]\tvalid_0's rmse: 0.547524\n",
      "[198]\tvalid_0's rmse: 0.547523\n",
      "[199]\tvalid_0's rmse: 0.54751\n",
      "[200]\tvalid_0's rmse: 0.547504\n",
      "Validation error: 0.5475037906584022\n",
      "Extracting fold...\n",
      "Preparing train...\n",
      "Preparing test...\n",
      "Training!\n",
      "[1]\tvalid_0's rmse: 0.966788\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tvalid_0's rmse: 0.903058\n",
      "[3]\tvalid_0's rmse: 0.847971\n",
      "[4]\tvalid_0's rmse: 0.800356\n",
      "[5]\tvalid_0's rmse: 0.759593\n",
      "[6]\tvalid_0's rmse: 0.724654\n",
      "[7]\tvalid_0's rmse: 0.695039\n",
      "[8]\tvalid_0's rmse: 0.669833\n",
      "[9]\tvalid_0's rmse: 0.648693\n",
      "[10]\tvalid_0's rmse: 0.631061\n",
      "[11]\tvalid_0's rmse: 0.616278\n",
      "[12]\tvalid_0's rmse: 0.604051\n",
      "[13]\tvalid_0's rmse: 0.593759\n",
      "[14]\tvalid_0's rmse: 0.585353\n",
      "[15]\tvalid_0's rmse: 0.578297\n",
      "[16]\tvalid_0's rmse: 0.57247\n",
      "[17]\tvalid_0's rmse: 0.567551\n",
      "[18]\tvalid_0's rmse: 0.563535\n",
      "[19]\tvalid_0's rmse: 0.56021\n",
      "[20]\tvalid_0's rmse: 0.557429\n",
      "[21]\tvalid_0's rmse: 0.555151\n",
      "[22]\tvalid_0's rmse: 0.553251\n",
      "[23]\tvalid_0's rmse: 0.551708\n",
      "[24]\tvalid_0's rmse: 0.550425\n",
      "[25]\tvalid_0's rmse: 0.549289\n",
      "[26]\tvalid_0's rmse: 0.548367\n",
      "[27]\tvalid_0's rmse: 0.547587\n",
      "[28]\tvalid_0's rmse: 0.546943\n",
      "[29]\tvalid_0's rmse: 0.546364\n",
      "[30]\tvalid_0's rmse: 0.545864\n",
      "[31]\tvalid_0's rmse: 0.545474\n",
      "[32]\tvalid_0's rmse: 0.545114\n",
      "[33]\tvalid_0's rmse: 0.544804\n",
      "[34]\tvalid_0's rmse: 0.544524\n",
      "[35]\tvalid_0's rmse: 0.544292\n",
      "[36]\tvalid_0's rmse: 0.544104\n",
      "[37]\tvalid_0's rmse: 0.543895\n",
      "[38]\tvalid_0's rmse: 0.543716\n",
      "[39]\tvalid_0's rmse: 0.543592\n",
      "[40]\tvalid_0's rmse: 0.54344\n",
      "[41]\tvalid_0's rmse: 0.543325\n",
      "[42]\tvalid_0's rmse: 0.543227\n",
      "[43]\tvalid_0's rmse: 0.543118\n",
      "[44]\tvalid_0's rmse: 0.543035\n",
      "[45]\tvalid_0's rmse: 0.542896\n",
      "[46]\tvalid_0's rmse: 0.542813\n",
      "[47]\tvalid_0's rmse: 0.542744\n",
      "[48]\tvalid_0's rmse: 0.542672\n",
      "[49]\tvalid_0's rmse: 0.542622\n",
      "[50]\tvalid_0's rmse: 0.542549\n",
      "[51]\tvalid_0's rmse: 0.542469\n",
      "[52]\tvalid_0's rmse: 0.542393\n",
      "[53]\tvalid_0's rmse: 0.542316\n",
      "[54]\tvalid_0's rmse: 0.542265\n",
      "[55]\tvalid_0's rmse: 0.542201\n",
      "[56]\tvalid_0's rmse: 0.5422\n",
      "[57]\tvalid_0's rmse: 0.542189\n",
      "[58]\tvalid_0's rmse: 0.542152\n",
      "[59]\tvalid_0's rmse: 0.542083\n",
      "[60]\tvalid_0's rmse: 0.54201\n",
      "[61]\tvalid_0's rmse: 0.541996\n",
      "[62]\tvalid_0's rmse: 0.541946\n",
      "[63]\tvalid_0's rmse: 0.541879\n",
      "[64]\tvalid_0's rmse: 0.54187\n",
      "[65]\tvalid_0's rmse: 0.541864\n",
      "[66]\tvalid_0's rmse: 0.541859\n",
      "[67]\tvalid_0's rmse: 0.541828\n",
      "[68]\tvalid_0's rmse: 0.541752\n",
      "[69]\tvalid_0's rmse: 0.541735\n",
      "[70]\tvalid_0's rmse: 0.541718\n",
      "[71]\tvalid_0's rmse: 0.541671\n",
      "[72]\tvalid_0's rmse: 0.541661\n",
      "[73]\tvalid_0's rmse: 0.541698\n",
      "[74]\tvalid_0's rmse: 0.541638\n",
      "[75]\tvalid_0's rmse: 0.541631\n",
      "[76]\tvalid_0's rmse: 0.541606\n",
      "[77]\tvalid_0's rmse: 0.541584\n",
      "[78]\tvalid_0's rmse: 0.541603\n",
      "[79]\tvalid_0's rmse: 0.541587\n",
      "[80]\tvalid_0's rmse: 0.54159\n",
      "[81]\tvalid_0's rmse: 0.541544\n",
      "[82]\tvalid_0's rmse: 0.541512\n",
      "[83]\tvalid_0's rmse: 0.541504\n",
      "[84]\tvalid_0's rmse: 0.54149\n",
      "[85]\tvalid_0's rmse: 0.541486\n",
      "[86]\tvalid_0's rmse: 0.541484\n",
      "[87]\tvalid_0's rmse: 0.541483\n",
      "[88]\tvalid_0's rmse: 0.541475\n",
      "[89]\tvalid_0's rmse: 0.541474\n",
      "[90]\tvalid_0's rmse: 0.54148\n",
      "[91]\tvalid_0's rmse: 0.541476\n",
      "[92]\tvalid_0's rmse: 0.541442\n",
      "[93]\tvalid_0's rmse: 0.541428\n",
      "[94]\tvalid_0's rmse: 0.541383\n",
      "[95]\tvalid_0's rmse: 0.541381\n",
      "[96]\tvalid_0's rmse: 0.541359\n",
      "[97]\tvalid_0's rmse: 0.541358\n",
      "[98]\tvalid_0's rmse: 0.541343\n",
      "[99]\tvalid_0's rmse: 0.541327\n",
      "[100]\tvalid_0's rmse: 0.541335\n",
      "[101]\tvalid_0's rmse: 0.541334\n",
      "[102]\tvalid_0's rmse: 0.541299\n",
      "[103]\tvalid_0's rmse: 0.541306\n",
      "[104]\tvalid_0's rmse: 0.541302\n",
      "[105]\tvalid_0's rmse: 0.541292\n",
      "[106]\tvalid_0's rmse: 0.541281\n",
      "[107]\tvalid_0's rmse: 0.541271\n",
      "[108]\tvalid_0's rmse: 0.541277\n",
      "[109]\tvalid_0's rmse: 0.541257\n",
      "[110]\tvalid_0's rmse: 0.541242\n",
      "[111]\tvalid_0's rmse: 0.541228\n",
      "[112]\tvalid_0's rmse: 0.541232\n",
      "[113]\tvalid_0's rmse: 0.541225\n",
      "[114]\tvalid_0's rmse: 0.54122\n",
      "[115]\tvalid_0's rmse: 0.54121\n",
      "[116]\tvalid_0's rmse: 0.541196\n",
      "[117]\tvalid_0's rmse: 0.541183\n",
      "[118]\tvalid_0's rmse: 0.54119\n",
      "[119]\tvalid_0's rmse: 0.541161\n",
      "[120]\tvalid_0's rmse: 0.541125\n",
      "[121]\tvalid_0's rmse: 0.541116\n",
      "[122]\tvalid_0's rmse: 0.541117\n",
      "[123]\tvalid_0's rmse: 0.541098\n",
      "[124]\tvalid_0's rmse: 0.541078\n",
      "[125]\tvalid_0's rmse: 0.541073\n",
      "[126]\tvalid_0's rmse: 0.541063\n",
      "[127]\tvalid_0's rmse: 0.541057\n",
      "[128]\tvalid_0's rmse: 0.541051\n",
      "[129]\tvalid_0's rmse: 0.541057\n",
      "[130]\tvalid_0's rmse: 0.541045\n",
      "[131]\tvalid_0's rmse: 0.541038\n",
      "[132]\tvalid_0's rmse: 0.541057\n",
      "[133]\tvalid_0's rmse: 0.541036\n",
      "[134]\tvalid_0's rmse: 0.541038\n",
      "[135]\tvalid_0's rmse: 0.541036\n",
      "[136]\tvalid_0's rmse: 0.541034\n",
      "[137]\tvalid_0's rmse: 0.541039\n",
      "[138]\tvalid_0's rmse: 0.541051\n",
      "[139]\tvalid_0's rmse: 0.541041\n",
      "[140]\tvalid_0's rmse: 0.541039\n",
      "[141]\tvalid_0's rmse: 0.54102\n",
      "[142]\tvalid_0's rmse: 0.541018\n",
      "[143]\tvalid_0's rmse: 0.54101\n",
      "[144]\tvalid_0's rmse: 0.541011\n",
      "[145]\tvalid_0's rmse: 0.541005\n",
      "[146]\tvalid_0's rmse: 0.541013\n",
      "[147]\tvalid_0's rmse: 0.540995\n",
      "[148]\tvalid_0's rmse: 0.540985\n",
      "[149]\tvalid_0's rmse: 0.540973\n",
      "[150]\tvalid_0's rmse: 0.540976\n",
      "[151]\tvalid_0's rmse: 0.540973\n",
      "[152]\tvalid_0's rmse: 0.540961\n",
      "[153]\tvalid_0's rmse: 0.540957\n",
      "[154]\tvalid_0's rmse: 0.540957\n",
      "[155]\tvalid_0's rmse: 0.540947\n",
      "[156]\tvalid_0's rmse: 0.540943\n",
      "[157]\tvalid_0's rmse: 0.540929\n",
      "[158]\tvalid_0's rmse: 0.540917\n",
      "[159]\tvalid_0's rmse: 0.540933\n",
      "[160]\tvalid_0's rmse: 0.540931\n",
      "[161]\tvalid_0's rmse: 0.540916\n",
      "[162]\tvalid_0's rmse: 0.540915\n",
      "[163]\tvalid_0's rmse: 0.540911\n",
      "[164]\tvalid_0's rmse: 0.540906\n",
      "[165]\tvalid_0's rmse: 0.540904\n",
      "[166]\tvalid_0's rmse: 0.540911\n",
      "[167]\tvalid_0's rmse: 0.54091\n",
      "[168]\tvalid_0's rmse: 0.540899\n",
      "[169]\tvalid_0's rmse: 0.540888\n",
      "[170]\tvalid_0's rmse: 0.54085\n",
      "[171]\tvalid_0's rmse: 0.540846\n",
      "[172]\tvalid_0's rmse: 0.540834\n",
      "[173]\tvalid_0's rmse: 0.540833\n",
      "[174]\tvalid_0's rmse: 0.540825\n",
      "[175]\tvalid_0's rmse: 0.540828\n",
      "[176]\tvalid_0's rmse: 0.540776\n",
      "[177]\tvalid_0's rmse: 0.540776\n",
      "[178]\tvalid_0's rmse: 0.540739\n",
      "[179]\tvalid_0's rmse: 0.540753\n",
      "[180]\tvalid_0's rmse: 0.540755\n",
      "[181]\tvalid_0's rmse: 0.540704\n",
      "[182]\tvalid_0's rmse: 0.540639\n",
      "[183]\tvalid_0's rmse: 0.540642\n",
      "[184]\tvalid_0's rmse: 0.540637\n",
      "[185]\tvalid_0's rmse: 0.540635\n",
      "[186]\tvalid_0's rmse: 0.540625\n",
      "[187]\tvalid_0's rmse: 0.540618\n",
      "[188]\tvalid_0's rmse: 0.540563\n",
      "[189]\tvalid_0's rmse: 0.540559\n",
      "[190]\tvalid_0's rmse: 0.540553\n",
      "[191]\tvalid_0's rmse: 0.540547\n",
      "[192]\tvalid_0's rmse: 0.540542\n",
      "[193]\tvalid_0's rmse: 0.540532\n",
      "[194]\tvalid_0's rmse: 0.540479\n",
      "[195]\tvalid_0's rmse: 0.540479\n",
      "[196]\tvalid_0's rmse: 0.54048\n",
      "[197]\tvalid_0's rmse: 0.540478\n",
      "[198]\tvalid_0's rmse: 0.54046\n",
      "[199]\tvalid_0's rmse: 0.540456\n",
      "[200]\tvalid_0's rmse: 0.540456\n",
      "Validation error: 0.5404564155124943\n",
      "Extracting fold...\n",
      "Preparing train...\n",
      "Preparing test...\n",
      "Training!\n",
      "[1]\tvalid_0's rmse: 0.962534\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tvalid_0's rmse: 0.90015\n",
      "[3]\tvalid_0's rmse: 0.846169\n",
      "[4]\tvalid_0's rmse: 0.799512\n",
      "[5]\tvalid_0's rmse: 0.759418\n",
      "[6]\tvalid_0's rmse: 0.725271\n",
      "[7]\tvalid_0's rmse: 0.696329\n",
      "[8]\tvalid_0's rmse: 0.671613\n",
      "[9]\tvalid_0's rmse: 0.650839\n",
      "[10]\tvalid_0's rmse: 0.633468\n",
      "[11]\tvalid_0's rmse: 0.618853\n",
      "[12]\tvalid_0's rmse: 0.606672\n",
      "[13]\tvalid_0's rmse: 0.596512\n",
      "[14]\tvalid_0's rmse: 0.588079\n",
      "[15]\tvalid_0's rmse: 0.581115\n",
      "[16]\tvalid_0's rmse: 0.575301\n",
      "[17]\tvalid_0's rmse: 0.570465\n",
      "[18]\tvalid_0's rmse: 0.566521\n",
      "[19]\tvalid_0's rmse: 0.563216\n",
      "[20]\tvalid_0's rmse: 0.560437\n",
      "[21]\tvalid_0's rmse: 0.558162\n",
      "[22]\tvalid_0's rmse: 0.556283\n",
      "[23]\tvalid_0's rmse: 0.554687\n",
      "[24]\tvalid_0's rmse: 0.553369\n",
      "[25]\tvalid_0's rmse: 0.552273\n",
      "[26]\tvalid_0's rmse: 0.551271\n",
      "[27]\tvalid_0's rmse: 0.550506\n",
      "[28]\tvalid_0's rmse: 0.549868\n",
      "[29]\tvalid_0's rmse: 0.549301\n",
      "[30]\tvalid_0's rmse: 0.54885\n",
      "[31]\tvalid_0's rmse: 0.54847\n",
      "[32]\tvalid_0's rmse: 0.548107\n",
      "[33]\tvalid_0's rmse: 0.547797\n",
      "[34]\tvalid_0's rmse: 0.547552\n",
      "[35]\tvalid_0's rmse: 0.547323\n",
      "[36]\tvalid_0's rmse: 0.54709\n",
      "[37]\tvalid_0's rmse: 0.546927\n",
      "[38]\tvalid_0's rmse: 0.546772\n",
      "[39]\tvalid_0's rmse: 0.546588\n",
      "[40]\tvalid_0's rmse: 0.546449\n",
      "[41]\tvalid_0's rmse: 0.546344\n",
      "[42]\tvalid_0's rmse: 0.546263\n",
      "[43]\tvalid_0's rmse: 0.546194\n",
      "[44]\tvalid_0's rmse: 0.546089\n",
      "[45]\tvalid_0's rmse: 0.546034\n",
      "[46]\tvalid_0's rmse: 0.545978\n",
      "[47]\tvalid_0's rmse: 0.545912\n",
      "[48]\tvalid_0's rmse: 0.545885\n",
      "[49]\tvalid_0's rmse: 0.545809\n",
      "[50]\tvalid_0's rmse: 0.545782\n",
      "[51]\tvalid_0's rmse: 0.545767\n",
      "[52]\tvalid_0's rmse: 0.545741\n",
      "[53]\tvalid_0's rmse: 0.545678\n",
      "[54]\tvalid_0's rmse: 0.545614\n",
      "[55]\tvalid_0's rmse: 0.545527\n",
      "[56]\tvalid_0's rmse: 0.545501\n",
      "[57]\tvalid_0's rmse: 0.545449\n",
      "[58]\tvalid_0's rmse: 0.545399\n",
      "[59]\tvalid_0's rmse: 0.545394\n",
      "[60]\tvalid_0's rmse: 0.54539\n",
      "[61]\tvalid_0's rmse: 0.545347\n",
      "[62]\tvalid_0's rmse: 0.545334\n",
      "[63]\tvalid_0's rmse: 0.54533\n",
      "[64]\tvalid_0's rmse: 0.545275\n",
      "[65]\tvalid_0's rmse: 0.545257\n",
      "[66]\tvalid_0's rmse: 0.54522\n",
      "[67]\tvalid_0's rmse: 0.545225\n",
      "[68]\tvalid_0's rmse: 0.545186\n",
      "[69]\tvalid_0's rmse: 0.545156\n",
      "[70]\tvalid_0's rmse: 0.545137\n",
      "[71]\tvalid_0's rmse: 0.545122\n",
      "[72]\tvalid_0's rmse: 0.545113\n",
      "[73]\tvalid_0's rmse: 0.545098\n",
      "[74]\tvalid_0's rmse: 0.545081\n",
      "[75]\tvalid_0's rmse: 0.545061\n",
      "[76]\tvalid_0's rmse: 0.545062\n",
      "[77]\tvalid_0's rmse: 0.545053\n",
      "[78]\tvalid_0's rmse: 0.544993\n",
      "[79]\tvalid_0's rmse: 0.544982\n",
      "[80]\tvalid_0's rmse: 0.544947\n",
      "[81]\tvalid_0's rmse: 0.544906\n",
      "[82]\tvalid_0's rmse: 0.544886\n",
      "[83]\tvalid_0's rmse: 0.544873\n",
      "[84]\tvalid_0's rmse: 0.544815\n",
      "[85]\tvalid_0's rmse: 0.544784\n",
      "[86]\tvalid_0's rmse: 0.544776\n",
      "[87]\tvalid_0's rmse: 0.544748\n",
      "[88]\tvalid_0's rmse: 0.544727\n",
      "[89]\tvalid_0's rmse: 0.544702\n",
      "[90]\tvalid_0's rmse: 0.544685\n",
      "[91]\tvalid_0's rmse: 0.544665\n",
      "[92]\tvalid_0's rmse: 0.544654\n",
      "[93]\tvalid_0's rmse: 0.544625\n",
      "[94]\tvalid_0's rmse: 0.544606\n",
      "[95]\tvalid_0's rmse: 0.544585\n",
      "[96]\tvalid_0's rmse: 0.544576\n",
      "[97]\tvalid_0's rmse: 0.544553\n",
      "[98]\tvalid_0's rmse: 0.544543\n",
      "[99]\tvalid_0's rmse: 0.544534\n",
      "[100]\tvalid_0's rmse: 0.544507\n",
      "[101]\tvalid_0's rmse: 0.544487\n",
      "[102]\tvalid_0's rmse: 0.544456\n",
      "[103]\tvalid_0's rmse: 0.54444\n",
      "[104]\tvalid_0's rmse: 0.544424\n",
      "[105]\tvalid_0's rmse: 0.544418\n",
      "[106]\tvalid_0's rmse: 0.544412\n",
      "[107]\tvalid_0's rmse: 0.544404\n",
      "[108]\tvalid_0's rmse: 0.544405\n",
      "[109]\tvalid_0's rmse: 0.544399\n",
      "[110]\tvalid_0's rmse: 0.544398\n",
      "[111]\tvalid_0's rmse: 0.544395\n",
      "[112]\tvalid_0's rmse: 0.544383\n",
      "[113]\tvalid_0's rmse: 0.544378\n",
      "[114]\tvalid_0's rmse: 0.544345\n",
      "[115]\tvalid_0's rmse: 0.544336\n",
      "[116]\tvalid_0's rmse: 0.544328\n",
      "[117]\tvalid_0's rmse: 0.544313\n",
      "[118]\tvalid_0's rmse: 0.544301\n",
      "[119]\tvalid_0's rmse: 0.5443\n",
      "[120]\tvalid_0's rmse: 0.544274\n",
      "[121]\tvalid_0's rmse: 0.544267\n",
      "[122]\tvalid_0's rmse: 0.544257\n",
      "[123]\tvalid_0's rmse: 0.544249\n",
      "[124]\tvalid_0's rmse: 0.544246\n",
      "[125]\tvalid_0's rmse: 0.544234\n",
      "[126]\tvalid_0's rmse: 0.544231\n",
      "[127]\tvalid_0's rmse: 0.54423\n",
      "[128]\tvalid_0's rmse: 0.544219\n",
      "[129]\tvalid_0's rmse: 0.544212\n",
      "[130]\tvalid_0's rmse: 0.544215\n",
      "[131]\tvalid_0's rmse: 0.5442\n",
      "[132]\tvalid_0's rmse: 0.544196\n",
      "[133]\tvalid_0's rmse: 0.544184\n",
      "[134]\tvalid_0's rmse: 0.54417\n",
      "[135]\tvalid_0's rmse: 0.544163\n",
      "[136]\tvalid_0's rmse: 0.544151\n",
      "[137]\tvalid_0's rmse: 0.54415\n",
      "[138]\tvalid_0's rmse: 0.544136\n",
      "[139]\tvalid_0's rmse: 0.544126\n",
      "[140]\tvalid_0's rmse: 0.544119\n",
      "[141]\tvalid_0's rmse: 0.544115\n",
      "[142]\tvalid_0's rmse: 0.544108\n",
      "[143]\tvalid_0's rmse: 0.544103\n",
      "[144]\tvalid_0's rmse: 0.544083\n",
      "[145]\tvalid_0's rmse: 0.544082\n",
      "[146]\tvalid_0's rmse: 0.544074\n",
      "[147]\tvalid_0's rmse: 0.544056\n",
      "[148]\tvalid_0's rmse: 0.544049\n",
      "[149]\tvalid_0's rmse: 0.544047\n",
      "[150]\tvalid_0's rmse: 0.544041\n",
      "[151]\tvalid_0's rmse: 0.544042\n",
      "[152]\tvalid_0's rmse: 0.54403\n",
      "[153]\tvalid_0's rmse: 0.544026\n",
      "[154]\tvalid_0's rmse: 0.54402\n",
      "[155]\tvalid_0's rmse: 0.544016\n",
      "[156]\tvalid_0's rmse: 0.544008\n",
      "[157]\tvalid_0's rmse: 0.544011\n",
      "[158]\tvalid_0's rmse: 0.543986\n",
      "[159]\tvalid_0's rmse: 0.543977\n",
      "[160]\tvalid_0's rmse: 0.543975\n",
      "[161]\tvalid_0's rmse: 0.54397\n",
      "[162]\tvalid_0's rmse: 0.543969\n",
      "[163]\tvalid_0's rmse: 0.543955\n",
      "[164]\tvalid_0's rmse: 0.543935\n",
      "[165]\tvalid_0's rmse: 0.543929\n",
      "[166]\tvalid_0's rmse: 0.543908\n",
      "[167]\tvalid_0's rmse: 0.543902\n",
      "[168]\tvalid_0's rmse: 0.543898\n",
      "[169]\tvalid_0's rmse: 0.543897\n",
      "[170]\tvalid_0's rmse: 0.543889\n",
      "[171]\tvalid_0's rmse: 0.543882\n",
      "[172]\tvalid_0's rmse: 0.543875\n",
      "[173]\tvalid_0's rmse: 0.543876\n",
      "[174]\tvalid_0's rmse: 0.543875\n",
      "[175]\tvalid_0's rmse: 0.543872\n",
      "[176]\tvalid_0's rmse: 0.54387\n",
      "[177]\tvalid_0's rmse: 0.543828\n",
      "[178]\tvalid_0's rmse: 0.543815\n",
      "[179]\tvalid_0's rmse: 0.543745\n",
      "[180]\tvalid_0's rmse: 0.543738\n",
      "[181]\tvalid_0's rmse: 0.543731\n",
      "[182]\tvalid_0's rmse: 0.543737\n",
      "[183]\tvalid_0's rmse: 0.543743\n",
      "[184]\tvalid_0's rmse: 0.543739\n",
      "[185]\tvalid_0's rmse: 0.543737\n",
      "[186]\tvalid_0's rmse: 0.543738\n",
      "[187]\tvalid_0's rmse: 0.543732\n",
      "[188]\tvalid_0's rmse: 0.543733\n",
      "[189]\tvalid_0's rmse: 0.543731\n",
      "[190]\tvalid_0's rmse: 0.543705\n",
      "[191]\tvalid_0's rmse: 0.543693\n",
      "[192]\tvalid_0's rmse: 0.543693\n",
      "[193]\tvalid_0's rmse: 0.54369\n",
      "[194]\tvalid_0's rmse: 0.543688\n",
      "[195]\tvalid_0's rmse: 0.543692\n",
      "[196]\tvalid_0's rmse: 0.543687\n",
      "[197]\tvalid_0's rmse: 0.543687\n",
      "[198]\tvalid_0's rmse: 0.543683\n",
      "[199]\tvalid_0's rmse: 0.543662\n",
      "[200]\tvalid_0's rmse: 0.543661\n",
      "Validation error: 0.5436609941589478\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for train_start, train_end, test_start, test_end in folds:\n",
    "    print(\"Extracting fold...\")\n",
    "    train = extract_by_date(df, train_start, train_end)\n",
    "    test= extract_by_date(df, test_start, test_end)\n",
    "    \n",
    "    print(\"Preparing train...\")\n",
    "    train, ext_cols = extend_dataset(train, items, stores)\n",
    "    x_cols = ext_cols + ['onpromotion'] + lagged_cols\n",
    "    train_X = train[x_cols]\n",
    "    train_y = train['unit_sales']\n",
    "    train_weights = get_weights(train['item_nbr'])\n",
    "    train_dataset = lgb.Dataset(train_X, label=train_y, weight=train_weights)\n",
    "    #del train\n",
    "    \n",
    "    print(\"Preparing test...\")\n",
    "    test, _ = extend_dataset(test, items, stores)\n",
    "    test_X = test[x_cols]\n",
    "    test_y = test['unit_sales']\n",
    "    test_weights = get_weights(test['item_nbr'])\n",
    "    test_dataset = lgb.Dataset(test_X, label=test_y, weight=test_weights, reference=train_dataset)\n",
    "    #del test\n",
    "    \n",
    "    print(\"Training!\")\n",
    "    bst = lgb.train(param, \n",
    "                    train_dataset, \n",
    "                    200,\n",
    "                    valid_sets=[test_dataset], \n",
    "                    early_stopping_rounds=10, \n",
    "                    verbose_eval=True, \n",
    "                    feature_name=x_cols, \n",
    "                    categorical_feature=ext_cols)\n",
    "    \n",
    "    test_y_pred = bst.predict(test_X)\n",
    "    error = NWRMSLE_log(test_y_pred, test_y, test_weights)\n",
    "    print('Validation error: {}'.format(error))\n",
    "    \n",
    "    results.append((bst.best_iteration, error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!telegram-send \"Lagged xgb validation finished. Results: $results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anatoliy\\Anaconda2\\envs\\py35\\lib\\site-packages\\lightgbm\\basic.py:1007: UserWarning: categorical_feature in Dataset is overrided. New categorical_feature is ['city', 'class', 'cluster', 'family', 'perishable', 'salary', 'state', 'type', 'weekday', 'weekend']\n",
      "  warnings.warn('categorical_feature in Dataset is overrided. New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    }
   ],
   "source": [
    "param['task'] = 'prediction'\n",
    "start = '2017-07-01'\n",
    "end = '2017-08-15'\n",
    "num_round = 100\n",
    "train = extract_by_date(df, start, end)\n",
    "train, ext_cols = extend_dataset(train, items, stores)\n",
    "x_cols = ext_cols + ['onpromotion'] + lagged_cols\n",
    "train_X = train[x_cols]\n",
    "train_y = train['unit_sales']\n",
    "train_weights = get_weights(train['item_nbr'])\n",
    "train_dataset = lgb.Dataset(train_X, label=train_y, weight=train_weights)\n",
    "bst = lgb.train(param, \n",
    "                train_dataset,\n",
    "                num_round,\n",
    "                feature_name=x_cols, \n",
    "                categorical_feature=ext_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anatoliy\\Anaconda2\\envs\\py35\\lib\\site-packages\\pandas\\core\\generic.py:2773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n",
      "C:\\Users\\Anatoliy\\Documents\\PROJECTS\\grocery\\model.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df_prev[colname] = df_prev['unit_sales']\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('./data/test_processed.csv', dtype=types)\n",
    "\n",
    "df_prev = extract_by_date(df, '2017-07-25', '2017-08-15')\n",
    "test = fill_mean_encoding(test, df_prev)\n",
    "test = fill_lagged(test, df_prev, 12, 18)\n",
    "\n",
    "test, ext_cols = extend_dataset(test, items, stores)\n",
    "x_cols = ext_cols + ['onpromotion'] + lagged_cols\n",
    "test_X = test[x_cols]\n",
    "test['unit_sales'] = bst.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.sort_values(by='id', inplace=True)\n",
    "test.ix[test.unit_sales < 0, 'unit_sales'] = 0\n",
    "test[['id', 'unit_sales']].to_csv('./submissions/lgb_lagged_0.54.csv.gz', float_format=\"%.4f\", index=False, compression='gzip')\n",
    "!telegram-send \"Submission done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./submissions/lgb_lagged_0.54.csv.gz', compression='gzip')\n",
    "submission.ix[submission.unit_sales < 0, 'unit_sales'] = 0\n",
    "submission['unit_sales'] = np.expm1(submission['unit_sales'])\n",
    "submission.to_csv('./submissions/lgb_lagged_0.54.csv.gz', float_format=\"%.4f\", index=False, compression='gzip')\n",
    "!telegram-send \"Submission done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
